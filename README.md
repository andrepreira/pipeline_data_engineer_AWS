# Pipeline de engenharia de dados com AWS

![Alt text](./assets/img/arquitetura.jpg?raw=true "Arquitetura pipeline")

‚úÖ Realizamos a ingest√£o de dados usando o Python e consumindo informa√ß√µes de uma API.
‚úÖ Validamos as informa√ß√µes da API.
‚úÖ Organizamos a estrutura√ß√£o da API em um formato Json para uma tabela em um banco de dados RDS na AWS.
‚úÖ Criamos um Data Lake usando o S3.
‚úÖ Provisionamos o AWS DMS para facilitar a ingest√£o no Data Lake.
‚úÖ Processamos dados usando Spark com o Amazon EMR.
‚úÖ Criamos tabelas Delta nas camadas "processed" e "curated".
‚úÖ Criamos crawlers para tornar as tabelas acess√≠veis no AWS Athena.
‚úÖ Configuramos Workgroups no Athena.
‚úÖ Discutimos casos de uso reais do Athena e suas aplicabilidades.
‚úÖ Realizamos o deploy do Amazon Redshift.
‚úÖ Realizamos o deploy de uma aplica√ß√£o Spark usando o Amazon EMR que escreve diretamente no Redshift.
‚úÖ Discutimos sobre a arquitetura Delta Lakehouse.
‚úÖ Abordamos muito sobre boas pr√°ticas.
‚úÖ Provionamos os recursos RDS, DMS, Crawlers, Redshift com Terraform.
üíé Mostramos a melhor forma de criar portf√≥lio para Engenheiros de Dados.


### Para rodar a aplica√ß√£o de ingest√£o fa√ßa:

1. Provisione o RDS PostgreSQL na AWS conforme abordado em aula.
2. Crie um banco de dados, por exemplo: coins
3. Configure o arquivo model.py com o nome da tabela a ser criada, exemplo: tb_coins.
4. Edite vari√°vel `db_string` com o endpoint do RDS na AWS.
5. Altere o arquivo app.py inserindo a chave da api como argumento da fun√ß√£o get_data (key)
6. Execute a aplica√ß√£o para consumo da API e persist√™ncia no banco de dados.


### Para rodar a aplica√ß√£o Spark:
1. Suba o Amazon EMR.

2. Navegue at√© o diret√≥rio `processing`

3. Copie a aplica√ß√£o para o servidor usando o comando `scp`, exemplo:

<code>scp -i ~/Downloads/pair-bootcamp.pem job-spark-app-emr-redshift.py hadoop@ec2-54-90-3-194.compute-1.amazonaws.com:/home/hadoop/ </code>

4. Conecte no servidor master usando `ssh`, exemplo:

<code> ssh -i ~/Downloads/pair-bootcamp.pem hadoop@ec2-54-90-3-194.compute-1.amazonaws.com </code>

*Obs*: Antes de executar a aplica√ß√£o verifique se o redshift est√° iniciado, caso n√£o esteja edite a aplica√ß√£o alterando a vari√°vel `flag_write_redshift` para false.

5. Execute o comando spark-submit, exemplo:
<code>spark-submit --packages io.delta:delta-core_2.12:2.0.0 --conf "spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension" --conf "spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog"  --jars /usr/share/aws/redshift/jdbc/RedshiftJDBC.jar,/usr/share/aws/redshift/spark-redshift/lib/spark-redshift.jar,/usr/share/aws/redshift/spark-redshift/lib/spark-avro.jar,/usr/share/aws/redshift/spark-redshift/lib/minimal-json.jar job-spark-app-emr-redshift.py</code>


### Para provisionar recursos com Terraform:
1. Navegue at√© o diret√≥rio `Terraform`

2. Instale o aplicativo `terraform`

3. Instale o aplicativo `aws-cli`. Veja esse link: https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html

4. Autentique na AWS com o comando:

<code>aws configure</code>

5. Antes de provisionar os recursos crie o backend para usar como *backend* e a tabela no Dynamodb.
